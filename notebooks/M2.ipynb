{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "25eNlCawQybd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import itertools\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torchvision\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "np.random.seed(1337)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = 'cuda:0'\n",
        "    torch.cuda.manual_seed(1337)\n",
        "else:\n",
        "    DEVICE = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VdIDwYPYPPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_DIM = 28 * 28\n",
        "Y_DIM = 10\n",
        "Z_DIM = 50\n",
        "HIDDEN_DIM1 = 600\n",
        "HIDDEN_DIM2 = 500\n",
        "INIT_VAR = 0.001\n",
        "\n",
        "\n",
        "def ohe_convert(y):\n",
        "    res = torch.zeros(len(y), Y_DIM)\n",
        "    res[torch.arange(len(y)), y.squeeze()] = 1\n",
        "    return res.to(y.device)\n",
        "\n",
        "\n",
        "class M2(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self.encoder_y = torch.nn.Sequential(\n",
        "            nn.Linear(X_DIM, HIDDEN_DIM2),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(HIDDEN_DIM2, HIDDEN_DIM2),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(HIDDEN_DIM2, Y_DIM)\n",
        "        )\n",
        "\n",
        "        self.encoder_z = torch.nn.Sequential(\n",
        "            nn.Linear(X_DIM + Y_DIM, HIDDEN_DIM2),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(HIDDEN_DIM2, HIDDEN_DIM2),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(HIDDEN_DIM2, Z_DIM * 2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(Y_DIM + Z_DIM, HIDDEN_DIM2),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(HIDDEN_DIM2, HIDDEN_DIM2),\n",
        "            nn.Softplus(),\n",
        "            nn.Linear(HIDDEN_DIM2, X_DIM)\n",
        "        )\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.ndim == 1:\n",
        "                p.data.fill_(0)\n",
        "            else:\n",
        "                p.data.normal_(0, INIT_VAR)\n",
        "\n",
        "                \n",
        "        self.p_z = torch.distributions.Normal(\n",
        "            torch.zeros(1, device=device), torch.ones(1, device=device)\n",
        "        )\n",
        "        self.p_y = torch.distributions.OneHotCategorical(\n",
        "            probs=torch.ones((1, Y_DIM), device=device) / Y_DIM)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        probs = self.encode_y(x).probs\n",
        "        return probs.max(dim=1)[1]\n",
        "\n",
        "\n",
        "    def encode_y(self, x):\n",
        "        return torch.distributions.OneHotCategorical(logits=self.encoder_y(x))\n",
        "\n",
        "\n",
        "    def encode_z(self, x, y):\n",
        "        res = self.encoder_z(torch.cat([x, y], axis=1))\n",
        "        means_z, logsigma_z = torch.chunk(res, 2, dim=-1)\n",
        "        return torch.distributions.Normal(means_z, torch.exp(logsigma_z))\n",
        "\n",
        "\n",
        "    def decode(self, y, z):\n",
        "        return torch.distributions.Bernoulli(\n",
        "            logits=self.decoder(torch.cat([y, z], axis=1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n4mwUQBYX_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 0.0003\n",
        "BETA1 = 0.9\n",
        "BETA2 = 0.999\n",
        "ALPHA = 0.1\n",
        "\n",
        "def train_M2(model: M2, dl_labeled, dl_unlabeled, dl_test, n_epochs, device):\n",
        "    def loss_func(x, y, z, p_x_yz, q_z_xy):\n",
        "        return p_x_yz.log_prob(x).sum(1) + \\\n",
        "            model.p_y.log_prob(y) + \\\n",
        "                model.p_z.log_prob(z).sum(1) - \\\n",
        "                    q_z_xy.log_prob(z).sum(1)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=LR, betas=(BETA1, BETA2))\n",
        "    n_batches = len(dl_labeled) + len(dl_unlabeled)\n",
        "\n",
        "\n",
        "    unlabeled_per_labeled = len(dl_unlabeled) // len(dl_labeled) + 1\n",
        "    train_loss_log = []\n",
        "    val_acc_log = []\n",
        "\n",
        "    for epoch in tqdm(range(n_epochs)):\n",
        "        model.train()\n",
        "        labeled_i = unlabeled_i = 0\n",
        "        dl_labeled_iterable = iter(dl_labeled)\n",
        "        dl_unlabeled_iterable = iter(dl_unlabeled)\n",
        "        for batch_id in range(n_batches):\n",
        "            unsupervised = bool(batch_id % unlabeled_per_labeled)\n",
        "            if not unsupervised and labeled_i == len(dl_labeled):\n",
        "                unsupervised = True\n",
        "            if unsupervised:\n",
        "                unlabeled_i += 1\n",
        "                x, _ = next(dl_unlabeled_iterable)\n",
        "                x = x.view(x.shape[0], -1).to(device)\n",
        "                q_y = model.encode_y(x)\n",
        "                loss = - q_y.entropy()\n",
        "                for y in q_y.enumerate_support():\n",
        "                    q_z_xy = model.encode_z(x, y)\n",
        "                    z = q_z_xy.rsample()\n",
        "                    p_x_yz = model.decode(y, z)\n",
        "                    L = loss_func(x, y, z, p_x_yz, q_z_xy)\n",
        "                    loss += q_y.log_prob(y).exp() * (-L)\n",
        "                \n",
        "            else:\n",
        "                labeled_i += 1\n",
        "                x, y = next(dl_labeled_iterable)\n",
        "                x = x.view(x.shape[0], -1).to(device)\n",
        "                y = ohe_convert(y).to(device)\n",
        "\n",
        "                q_y = model.encode_y(x)\n",
        "\n",
        "                q_z_xy = model.encode_z(x, y)\n",
        "                z = q_z_xy.rsample()\n",
        "                p_x_yz = model.decode(y, z)\n",
        "                loss = -loss_func(x, y, z, p_x_yz, q_z_xy)\n",
        "                loss -= ALPHA * len(dl_labeled) * q_y.log_prob(y)\n",
        "\n",
        "            loss = loss.mean(0)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        train_loss_log.append(loss.data.item())\n",
        "        val_acc_log.append(evaluate(model, dl_test, device))\n",
        "                # print (f'Epoch [{epoch + 1}/{n_epochs}], Loss: {loss.data.item()}')\n",
        "        plot_history(train_loss_log, val_acc_log, epoch + 1)\n",
        "        # torch.save(model.state_dict(), '../log/')\n",
        "    return train_loss_log, val_acc_log\n",
        "\n",
        "\n",
        "def plot_history(train_history, val_history, epoch):\n",
        "    clear_output()\n",
        "    plt.figure()\n",
        "    plt.title('Train loss')\n",
        "    plt.plot(np.arange(len(train_history)), train_history, label='train', zorder=1)\n",
        "    plt.xlabel('train steps')\n",
        "    \n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title('Val accuracy')\n",
        "    plt.plot(np.arange(len(val_history)), val_history, label='val', c='orange', zorder=1)\n",
        "    plt.xlabel('train steps')\n",
        "    \n",
        "    plt.legend(loc='best')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dl, device):\n",
        "    model.eval()\n",
        "    accurate_preds = 0\n",
        "    all_count = 0\n",
        "    for x, y in dl:\n",
        "        all_count += x.shape[0]\n",
        "        x = x.to(device).view(x.shape[0], -1)\n",
        "        y = y.to(device)\n",
        "        preds = model(x)\n",
        "        accurate_preds += (preds == y).sum().item()\n",
        "\n",
        "    return accurate_preds / all_count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EkLYIYCYg4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(n_labeled, batch_size=64):\n",
        "\n",
        "    transforms = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    train_labeled = torchvision.datasets.MNIST(PATH, download=True, train=True, transform=transforms)\n",
        "    train_unlabeled = copy.deepcopy(train_labeled)\n",
        "\n",
        "    n_classes = np.unique(train_labeled.train_labels).size\n",
        "    n_labeled_class = n_labeled // n_classes\n",
        "\n",
        "    x_labeled, y_labeled, x_unlabeled, y_unlabeled = map(lambda x: [], range(4))\n",
        "    for i in range(n_classes):\n",
        "        mask = train_labeled.train_labels == i\n",
        "        x_masked = train_labeled.data[mask]\n",
        "        y_masked = train_labeled.train_labels[mask]\n",
        "        np.random.shuffle(x_masked)\n",
        "\n",
        "        x_labeled.append(x_masked[:n_labeled_class])\n",
        "        x_unlabeled.append(x_masked[n_labeled_class:])\n",
        "        y_labeled.append(y_masked[:n_labeled_class])\n",
        "        y_unlabeled.append(y_masked[n_labeled_class:])\n",
        "\n",
        "    \n",
        "    train_unlabeled.data = torch.cat(x_unlabeled).squeeze()\n",
        "    train_unlabeled.labels = torch.cat(y_unlabeled)\n",
        "    train_labeled.data = torch.cat(x_labeled).squeeze()\n",
        "    train_labeled.labels = torch.cat(y_labeled)\n",
        "\n",
        "    dl_train_labeled = torch.utils.data.DataLoader(train_labeled, batch_size=batch_size, shuffle=True)\n",
        "    dl_train_unlabeled = torch.utils.data.DataLoader(train_unlabeled, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test = torchvision.datasets.MNIST(PATH, download=True, train=False, transform=transforms)\n",
        "    dl_test = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return dl_train_labeled, dl_train_unlabeled, dl_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRXQHFscaLV3",
        "colab_type": "code",
        "outputId": "440be7e6-eaae-496c-fc09-9904e5cbe16c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "dl_labeled, dl_unlabeled, dl_test = load_data(3000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjMAvR9jQ0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV48Xv8qnVc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = M2(DEVICE).to(DEVICE)\n",
        "train_loss_log, val_acc_log = train_M2(model, dl_labeled, dl_unlabeled, dl_test, N_EPOCHS, DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlJpp-YZz8CG",
        "colab_type": "code",
        "outputId": "b9206931-9720-498e-89ee-d69b81b737b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_loss_log[-1], val_acc_log[-1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105.78288269042969, 0.098)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    }
  ]
}